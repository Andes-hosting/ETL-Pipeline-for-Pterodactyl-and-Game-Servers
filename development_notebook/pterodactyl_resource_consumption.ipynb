{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pterodactyl Minecraft Resource Consumption\n",
    "\n",
    "### Index\n",
    "\n",
    "- Install requierements\n",
    "- Import libraries and setup key variables\n",
    "- Define functions\n",
    "- Get Pterodactyl Utilization Information\n",
    "- Load data into the Postgres database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install requierements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and setup key variables\n",
    "Remember to add you own credentials in the .env file for them to be loaded here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os\n",
    "from sqlalchemy import create_engine, text\n",
    "from pydactyl import PterodactylClient\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Load .env file credentials\n",
    "load_dotenv()\n",
    "\n",
    "# Database connection\n",
    "host = os.getenv('POSTGRES_HOST')\n",
    "port = os.getenv('POSTGRES_PORT')\n",
    "database = os.getenv('POSTGRES_DATABASE')\n",
    "username = os.getenv('POSTGRES_USERNAME')\n",
    "password = os.getenv('POSTGRES_PASSWORD')\n",
    "connection = f'postgresql://{username}:{password}@{host}:{port}/{database}'\n",
    "\n",
    "# Pterodactyl connection\n",
    "pterodactyl_url = os.getenv('PTERODACTYL_URL')\n",
    "application_api_key = os.getenv('PTERODACTYL_APP_KEY')\n",
    "client_api_key = os.getenv('PTERODACTYL_CLI_KEY')\n",
    "\n",
    "# Connecto to Pterodactyl Application API\n",
    "api_app = PterodactylClient(pterodactyl_url, application_api_key, debug=False)\n",
    "# Connecto to Pterodactyl Client API\n",
    "api_cli = PterodactylClient(pterodactyl_url, client_api_key, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform from bytes to megabytes\n",
    "def bytes_to_megabytes(value):\n",
    "    return value / (1024**2) if isinstance(value, (int, float)) else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Pterodactyl Utilization Information\n",
    "About: current state, memory bytes, cpu absolute, disk bytes, network rx/tx bytes, uptime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting variables\n",
    "SCHEMA = 'pterodactyl'\n",
    "WINDOW_EXTRACTION_TIME = 30 # the time window in which it recieves data from the servers [seconds]\n",
    "BREAK_TIME = 10 # the time it takes to rest after getting data from all servers [seconds]\n",
    "WAITING_TIME = 1 # the time it takes to rest after getting date from each server [seconds]\n",
    "\n",
    "# Define the schema and extrat all uuid from every server from postgres\n",
    "engine = create_engine(connection)\n",
    "with engine.connect() as conn:\n",
    "    list_of_uuid = conn.execute(text(f'SELECT servers.uuid FROM {SCHEMA}.servers  WHERE servers.is_active = true'))\n",
    "    result = list_of_uuid.fetchall()\n",
    "    list_servers = [uuid for uuid, in result] #remove the tuples of uuid from results\n",
    "\n",
    "# Extract the data from every uuid in the postgres database\n",
    "all_utilizations = []\n",
    "start_time = time.time()\n",
    "while (time.time() - start_time) < WINDOW_EXTRACTION_TIME:\n",
    "    print(time.time() - start_time)\n",
    "    for server in list_servers:\n",
    "        try:\n",
    "            consumption = api_cli.client.servers.get_server_utilization(server)\n",
    "            consumption.update({'identifier': server[:8]})\n",
    "            all_utilizations.append(consumption)\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(WAITING_TIME)\n",
    "    time.sleep(BREAK_TIME)\n",
    "print(time.time() - start_time)\n",
    "\n",
    "# Create the dataframe and extract data from resources\n",
    "df_consumptions = pd.DataFrame(all_utilizations)\n",
    "df_consumptions['status'] = df_consumptions['current_state'].replace({'running': True, 'offline': False})\n",
    "df_consumptions['cpu'] = df_consumptions['resources'].apply(lambda x: x.get('cpu_absolute', None))\n",
    "df_consumptions['ram'] = df_consumptions['resources'].apply(lambda x: bytes_to_megabytes(x.get('memory_bytes', None)))\n",
    "df_consumptions['disk'] = df_consumptions['resources'].apply(lambda x: bytes_to_megabytes(x.get('disk_bytes', None)))\n",
    "df_consumptions['capture_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_consumptions = df_consumptions[['identifier', 'status', 'ram', 'cpu', 'disk', 'capture_time']]\n",
    "\n",
    "# Group by 'identifier' and calculate mean, std, min, and max for each group\n",
    "final_utilization_df = df_consumptions.groupby('identifier').agg({\n",
    "    'status': ['first'],  # Include 'current_state' in the aggregation\n",
    "    'ram': ['mean', 'std', 'min', 'max'],\n",
    "    'cpu': ['mean', 'std', 'min', 'max'],\n",
    "    'disk': ['mean', 'std', 'min', 'max'],\n",
    "    'capture_time': ['first']  \n",
    "}).reset_index()\n",
    "\n",
    "# Flatten the column names\n",
    "final_utilization_df.columns = ['_'.join(col).strip() for col in final_utilization_df.columns.values]\n",
    "\n",
    "# Rename columns\n",
    "final_utilization_df = final_utilization_df.rename(columns={'identifier_': 'server_identifier'})\n",
    "final_utilization_df = final_utilization_df.rename(columns={'status_first': 'status'})\n",
    "final_utilization_df = final_utilization_df.rename(columns={'capture_time_first': 'capture_time'})\n",
    "print(final_utilization_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into Data Warehouse (Postgres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database and upload all new logs into table\n",
    "engine = create_engine(connection)\n",
    "with engine.connect() as conn:\n",
    "\n",
    "# Start a new transaction\n",
    "    trans = conn.begin()\n",
    "\n",
    "    try:\n",
    "        # Load all new activity into postgres\n",
    "        final_utilization_df.to_sql(name = 'utilization', schema = SCHEMA, con = conn, if_exists='append', index=False)\n",
    "        # Commit the transaction\n",
    "        trans.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        # Rollback the transaction on exception\n",
    "        print('!!! [ERROR IN DATABASE QUERIES] !!!')\n",
    "        trans.rollback()\n",
    "        print('Transaction has been rolled back')\n",
    "        print(f'Error occurred during transaction:\\n{e}')\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
