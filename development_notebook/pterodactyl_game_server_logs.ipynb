{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline for Pterodactyl Game Servers Logs\n",
    "\n",
    "### Index\n",
    "\n",
    "- Install requierements\n",
    "- Import libraries and setup key variables\n",
    "- Setup directories, functions and folder creation\n",
    "- Extract logs from each active Minecraft Server\n",
    "- Transformation from logs data into information\n",
    "- Load processed data into Data Warehouse (Postgres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install requierements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and setup key variables\n",
    "Remember to add you own credentials in the .env file for them to be loaded here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, zipfile, requests, gzip, csv, os, re\n",
    "from sqlalchemy import create_engine, text\n",
    "from pydactyl import PterodactylClient\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# Load .env file credentials\n",
    "load_dotenv()\n",
    "\n",
    "# Database connection\n",
    "host = os.getenv('POSTGRES_HOST')\n",
    "port = os.getenv('POSTGRES_PORT')\n",
    "database = os.getenv('POSTGRES_DATABASE')\n",
    "username = os.getenv('POSTGRES_USERNAME')\n",
    "password = os.getenv('POSTGRES_PASSWORD')\n",
    "connection = f'postgresql://{username}:{password}@{host}:{port}/{database}'\n",
    "\n",
    "# Pterodactyl connection\n",
    "pterodactyl_url = os.getenv('PTERODACTYL_URL')\n",
    "application_api_key = os.getenv('PTERODACTYL_APP_KEY')\n",
    "client_api_key = os.getenv('PTERODACTYL_CLI_KEY')\n",
    "\n",
    "# Connecto to Pterodactyl Application API\n",
    "api_app = PterodactylClient(pterodactyl_url, application_api_key, debug=False)\n",
    "# Connecto to Pterodactyl Client API\n",
    "api_cli = PterodactylClient(pterodactyl_url, client_api_key, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup directories, functions and folder creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up schema name in database\n",
    "schema = 'pterodactyl'\n",
    "\n",
    "# Setup directories\n",
    "pwd = os.getcwd() #os.path.dirname(os.path.realpath(__file__)) this is used for .py files\n",
    "raw_logs_folder = os.path.join(pwd, 'raw_logs')\n",
    "\n",
    "# Create new folder if not exists\n",
    "def mkdir(folder_dir):\n",
    "    if not os.path.exists(folder_dir):\n",
    "        os.makedirs(os.path.join(pwd, folder_dir))\n",
    "\n",
    "# Sort a list of logs names\n",
    "def sort_list_logs(logs):\n",
    "    def logs_modifications(log):\n",
    "        # remove the '-' from the log\n",
    "        date_part, number_part = log.split('-')[:3], log.split('-')[3]\n",
    "        # Join the parts of date to transform\n",
    "        date_log = '-'.join(date_part)\n",
    "        # remove the '.log' extension and pass the number to int\n",
    "        number_log = int(number_part.split('.')[0])\n",
    "        # return the date and number for sorting\n",
    "        return date_log, number_log\n",
    "\n",
    "    # use the logs_modifications function to sort the logs by date and number\n",
    "    sorted_logs = sorted(logs, key=logs_modifications)\n",
    "    return sorted_logs\n",
    "\n",
    "# Get last index from a list when matching with an element\n",
    "def last_index(list, element):\n",
    "    try:\n",
    "        return len(list) - 1 - list[::-1].index(element)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"'{element}' not found in the list\")\n",
    "\n",
    "# Extract the compressed files (.gz and .zip)    \n",
    "def extract_compressed_file(compressed_file_path, decompressed_folder_path):\n",
    "    if compressed_file_path.endswith('.gz'):\n",
    "        with gzip.open(compressed_file_path, 'rb') as compressed_file:\n",
    "            with open(decompressed_folder_path, 'wb') as decompressed_file:\n",
    "                decompressed_file.write(compressed_file.read())\n",
    "    elif compressed_file_path.endswith('.zip'):\n",
    "        with zipfile.ZipFile(compressed_file_path, 'r') as zip_file:\n",
    "            zip_file.extractall(os.path.dirname(decompressed_folder_path))\n",
    "\n",
    "# Create folder if not exist\n",
    "mkdir(raw_logs_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_logs(id, identifier, egg, last_log, staging_folder):\n",
    "    eggs_minecraft = ('Vanilla Minecraft', 'Forge Minecraft', 'Paper', 'Spigot', 'Mohist')\n",
    "    eggs_ark = ('Ark: Survival Evolved',)\n",
    "    all_eggs = eggs_minecraft + eggs_ark\n",
    "\n",
    "    folder_logs = {\n",
    "        'Vanilla Minecraft': '/logs/',\n",
    "        'Forge Minecraft': '/logs/',\n",
    "        'Paper': '/logs/',\n",
    "        'Spigot': '/logs/',\n",
    "        'Mohist': '/logs/',\n",
    "        'Ark: Survival Evolved': '/ShooterGame/Saved/Logs/'\n",
    "    }\n",
    "\n",
    "    log_pattern = {\n",
    "        'Vanilla Minecraft': r'^\\d{4}-\\d{2}-\\d{2}-\\d.*', # yyyy-mm-dd-n*\n",
    "        'Forge Minecraft': r'^\\d{4}-\\d{2}-\\d{2}-\\d.*',\n",
    "        'Paper': r'^\\d{4}-\\d{2}-\\d{2}-\\d.*',\n",
    "        'Spigot': r'^\\d{4}-\\d{2}-\\d{2}-\\d.*',\n",
    "        'Mohist': r'^\\d{4}-\\d{2}-\\d{2}-\\d.*',\n",
    "        'Ark: Survival Evolved': r'^\\d{4}-\\d{2}-\\d{2}-\\d.*' #ServerGame.316.2023.11.17_19.25.10.log or ShooterGame-backup-2023.11.16-21.13.36.log - not sure \n",
    "    }\n",
    "\n",
    "    compression_ext = ('.gz', '.zip')\n",
    "\n",
    "    if egg in all_eggs:\n",
    "        # Create a folder as staging area for each server\n",
    "        folder_server_dir = os.path.join(staging_folder, id)\n",
    "        mkdir(folder_server_dir)\n",
    "\n",
    "        # Get the last log date to download only necessary logs\n",
    "        if last_log is not None:\n",
    "            last_log_date = last_log.strftime('%Y-%m-%d')\n",
    "        else:\n",
    "            # Set a default value to download all logs in case when last log date is not available\n",
    "            last_log_date = '2000-01-01'\n",
    "\n",
    "        # Get a list of the logs inside the server\n",
    "        try:\n",
    "            log_files = api_cli.client.servers.files.list_files(identifier, folder_logs[egg])\n",
    "            log_files_data = log_files.get('data', [])\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response.status_code == 404 or e.response.status_code == 504:\n",
    "                print(f\"Server {identifier} not found in Pterodactyl.\")\n",
    "                return None # Skip this and continue to the next iteration\n",
    "\n",
    "        # If log_files_data is not empty, sort the list of logs based on the date naming\n",
    "        if log_files_data:\n",
    "            list_logs = [file['attributes']['name'] for file in log_files_data if re.match(log_pattern[egg], file['attributes']['name'])]\n",
    "            sorted_list_logs = sort_list_logs(list_logs)\n",
    "            sorted_list_logs_date = [item[:10] for item in sorted_list_logs]\n",
    "        else:\n",
    "            print(f\"No log files found in server {identifier} with the client API.\")\n",
    "\n",
    "        # Select only a list of downloadable_logs which are new; after the last_log_date\n",
    "        try:\n",
    "            index_last_log = last_index(sorted_list_logs_date, last_log_date)\n",
    "        except:\n",
    "            index_last_log = -1\n",
    "        downloadable_logs = sorted_list_logs[index_last_log + 1:]\n",
    "\n",
    "        # Download all logs in the list downloadable_logs\n",
    "        list_download = [api_cli.client.servers.files.download_file(server_info[0], f'/logs/{log}') for log in downloadable_logs]\n",
    "        if list_download:\n",
    "            [urllib.request.urlretrieve(list_download[i], os.path.join(folder_server_dir, list_logs[i])) for i in range(len(list_download))]\n",
    "        print(f'Files downloaded: {len(list_download)}')\n",
    "\n",
    "        # Uncompressing files if needed\n",
    "        for filename in os.listdir(folder_server_dir):\n",
    "            if filename.endswith(compression_ext):\n",
    "                compressed_file_path = os.path.join(folder_server_dir, filename)\n",
    "                decompressed_file_path = os.path.splitext(compressed_file_path)[0]  # Remove the last extension\n",
    "\n",
    "                # Uncompress the file\n",
    "                extract_compressed_file(compressed_file_path, decompressed_file_path)\n",
    "\n",
    "                # Delete the compressed file\n",
    "                os.remove(compressed_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract logs from each active Minecraft Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get server information from database\n",
    "engine = create_engine(connection)\n",
    "with engine.connect() as conn:\n",
    "    list_servers = conn.execute(text(f'SELECT servers.id, servers.identifier, eggs.name AS egg, last_log_date.last_date FROM {schema}.servers JOIN {schema}.eggs ON eggs.id = servers.egg_id LEFT JOIN {schema}.last_log_date ON last_log_date.server_identifier = servers.identifier WHERE servers.is_active = true'))\n",
    "\n",
    "# Save server information from the query into a dataframe\n",
    "df_servers = pd.DataFrame(list_servers.fetchall(), columns=list_servers.keys())\n",
    "\n",
    "# Download the latest logs based on last_log and egg into the staging folder\n",
    "for server_info in df_servers.iterrows():\n",
    "    download_logs(id = server_info[1]['id'], identifier = server_info[1]['identifier'], egg = server_info[1]['egg'], last_log = server_info[1]['last_date'], staging_folder = raw_logs_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation from logs data to information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_logs(id, path_dir, df_servers):\n",
    "\n",
    "    eggs_minecraft = ('Vanilla Minecraft', 'Forge Minecraft', 'Paper', 'Spigot', 'Mohist')\n",
    "    eggs_ark = ('Ark: Survival Evolved',)\n",
    "    all_eggs = eggs_minecraft + eggs_ark\n",
    "\n",
    "    # Recognize current egg based on server id (folder name)\n",
    "    current_egg = df_servers.loc[df_servers['id'] == folder]['egg'][0]\n",
    "\n",
    "    # Check if the egg is available for processing\n",
    "    if current_egg in all_eggs:\n",
    "\n",
    "        # Read all logs as one\n",
    "        log_files = [log for log in os.listdir(folder_server_dir) if log.endswith('.log')]\n",
    "        log_files = sort_list_logs(log_files)\n",
    "\n",
    "        if log_files:\n",
    "\n",
    "            all_logs = \"\"\n",
    "            for log_file in log_files:\n",
    "                with open(os.path.join(folder_server_dir, log_file), 'r', encoding='utf-8') as file:\n",
    "                    log_contents = file.read().split('\\n')\n",
    "                    log_contents = \"\\n\".join([f'[{log_file[:-(2+len(log_extension))]}] ' + line for line in log_contents if line.strip() != \"\"])\n",
    "                    all_logs += log_contents + \"\\n\"\n",
    "\n",
    "            # Transformation for all Minecraft Eggs\n",
    "            if current_egg in eggs_minecraft:\n",
    "\n",
    "                # Transform information in meaningful information\n",
    "                pattern = r'\\[(\\d{4}-\\d{2}-\\d{2})\\] \\[.*?(\\d{2}:\\d{2}:\\d{2}).*?\\].*?: (.*)'\n",
    "                matches = re.findall(pattern, all_logs)\n",
    "\n",
    "                # Create a list of dictionaries to store the extracted data\n",
    "                log_data = [{'server_id': folder, 'date': match[0], 'time': match[1], 'information': match[2]} for match in matches]\n",
    "                # Create a dataframe of the logs\n",
    "                df_logs = pd.DataFrame(log_data)\n",
    "                df_logs['user'] = None\n",
    "                df_logs['activity'] = None\n",
    "\n",
    "                # Add information when server started\n",
    "                index=df_logs[df_logs['information'].str.startswith(\"Starting minecraft server version\")].index\n",
    "                if not index.empty:\n",
    "                    df_logs.loc[index, 'user'] = 'server'\n",
    "                    df_logs.loc[index, 'activity'] = 'start'\n",
    "\n",
    "                # Add information when server stopped\n",
    "                index=df_logs[df_logs['information'].str.startswith(\"Stopping the server\")].index\n",
    "                if not index.empty:\n",
    "                    df_logs.loc[index, 'user'] = 'server'\n",
    "                    df_logs.loc[index, 'activity'] = 'stop'\n",
    "\n",
    "                # Add information when user login\n",
    "                users_logged_in = df_logs['information'].str.extract(r'(\\w+)\\[.*\\] logged in with entity id \\d+ at \\(.*\\)')\n",
    "                df_logs.update(users_logged_in.rename(columns={0: 'user'}), overwrite=False)\n",
    "                df_logs.loc[users_logged_in.dropna().index, 'activity'] = 'login'\n",
    "\n",
    "                # Add information when user logout\n",
    "                users_logged_out = df_logs['information'].str.extract(r'(\\w+) left the game')\n",
    "                df_logs.update(users_logged_out.rename(columns={0: 'user'}), overwrite=False)\n",
    "                df_logs.loc[users_logged_out.dropna().index, 'activity'] = 'logout'\n",
    "\n",
    "                # Add information when user chat\n",
    "                users_chat = df_logs['information'].str.extract(r'<(\\w+)> .*')\n",
    "                df_logs.update(users_chat.rename(columns={0: 'user'}), overwrite=False)\n",
    "                df_logs.loc[users_chat.dropna().index, 'activity'] = 'chat'\n",
    "\n",
    "                # Add information when user get an achievement\n",
    "                users_achievement = df_logs['information'].str.extract(r'(\\w+) has made the advancement')\n",
    "                df_logs.update(users_achievement.rename(columns={0: 'user'}), overwrite=False)\n",
    "                df_logs.loc[users_achievement.dropna().index, 'activity'] = 'achievement'\n",
    "\n",
    "                # Rows with no server/user activity is deleted\n",
    "                df_logs = df_logs.dropna(subset=['activity'])\n",
    "\n",
    "            # Transformation for all Ark Eggs\n",
    "            elif current_egg in eggs_ark:\n",
    "                df_logs = pd.DataFrame()\n",
    "\n",
    "    # Return an empty dataframe if the egg is not supported\n",
    "    else:\n",
    "        df_logs = pd.DataFrame()\n",
    "\n",
    "    return df_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_logs = pd.DataFrame(columns=['server_id', 'date', 'time', 'information', 'user', 'activity'])\n",
    "\n",
    "for folder in os.listdir(raw_logs_folder): \n",
    "    folder_server_dir = os.path.join(raw_logs_folder, folder)\n",
    "\n",
    "    # \n",
    "    df_logs = transform_logs(id = folder, path_dir = folder_server_dir, df_servers = df_servers)\n",
    "    # Save every log in df_all_logs for each iteration\n",
    "    df_all_logs = pd.concat([df_all_logs, df_logs], ignore_index=True)\n",
    "\n",
    "    # Delete all logs inside each server folder\n",
    "    for folder in os.listdir(raw_logs_folder):\n",
    "        folder_server_dir = os.path.join(raw_logs_folder, folder)\n",
    "        [os.remove(os.path.join(folder_server_dir, log)) for log in os.listdir(folder_server_dir) if log.endswith(log_extension)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processed data into Data Warehouse (Postgres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set table name\n",
    "table = 'activity'\n",
    "\n",
    "# Connect to database and upload all new logs into table\n",
    "engine = create_engine(connection)\n",
    "with engine.connect() as conn:\n",
    "\n",
    "# Start a new transaction\n",
    "    trans = conn.begin()\n",
    "\n",
    "    try:\n",
    "        # Load all new activity into postgres\n",
    "        df_all_logs.to_sql(name = table, schema = schema, con = conn, if_exists='append', index=False)\n",
    "        # Commit the transaction\n",
    "        trans.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        # Rollback the transaction on exception\n",
    "        print('!!! [ERROR IN DATABASE QUERIES] !!!')\n",
    "        trans.rollback()\n",
    "        print('Transaction has been rolled back')\n",
    "        print(f'Error occurred during transaction:\\n{e}')\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
