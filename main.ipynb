{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline for Pterodactyl Minecraft Servers Analysis\n",
    "\n",
    "## Index\n",
    "\n",
    "- Install requierements\n",
    "- Import libraries and setup key variables\n",
    "- Setup folders and directories\n",
    "- Define functions\n",
    "- Folder creation if not exist\n",
    "- Get Pterodactyl Application information\n",
    "- Upload csv table files into Postgres\n",
    "- Extract logs from each active Minecraft Server\n",
    "- Transformation from logs data into information\n",
    "- Load processed data into Data Warehouse (Postgres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install requierements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and setup key variables\n",
    "Remember to add you own credentials in the .env file for them to be loaded here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, psycopg2, pyarrow, json, csv, gzip, re, os\n",
    "from sqlalchemy import create_engine, text\n",
    "from pydactyl import PterodactylClient\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Load .env file credentials\n",
    "load_dotenv()\n",
    "\n",
    "# Database connection\n",
    "host = os.getenv('POSTGRES_HOST')\n",
    "port = os.getenv('POSTGRES_PORT')\n",
    "dbname = os.getenv('POSTGRES_DBNAME')\n",
    "user = os.getenv('POSTGRES_USER')\n",
    "password = os.getenv('POSTGRES_PASSWORD')\n",
    "connection = f'postgresql://{user}:{password}@{host}:{port}/{dbname}'\n",
    "\n",
    "# Pterodactyl connection\n",
    "pterodactyl_url = os.getenv('PTERODACTYL_URL')\n",
    "application_api_key = os.getenv('PTERODACTYL_APPLICATION_API_KEY')\n",
    "client_api_key = os.getenv('PTERODACTYL_CLIENT_API_KEY')\n",
    "\n",
    "# Connecto to Pterodactyl Application API\n",
    "api_app = PterodactylClient(pterodactyl_url, application_api_key, debug=False)\n",
    "# Connecto to Pterodactyl Client API\n",
    "api_cli = PterodactylClient(pterodactyl_url, client_api_key, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup folders and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = os.getcwd() #os.path.dirname(os.path.realpath(__file__)) this is used for .py files\n",
    "server_app_folder = os.path.join(pwd, 'server_app_data')\n",
    "raw_logs_folder = os.path.join(pwd, 'raw_logs')\n",
    "#output_folder = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new folder if not exists\n",
    "def mkdir(folder_dir):\n",
    "    if not os.path.exists(folder_dir):\n",
    "        os.makedirs(os.path.join(pwd, folder_dir))\n",
    "\n",
    "# Export data into a .csv file\n",
    "def save_to_csv(data, filename):\n",
    "    with open(os.path.join(pwd, server_app_folder, filename), 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = data[0][0].keys()  # Assuming the data is not empty\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "        for record in data:\n",
    "            for item in record:\n",
    "                writer.writerow(item)\n",
    "\n",
    "# Sort a list of logs names\n",
    "def sort_list_logs(logs):\n",
    "    def logs_modifications(log):\n",
    "        # remove the '-' from the log\n",
    "        date_part, number_part = log.split('-')[:3], log.split('-')[3]\n",
    "        # Join the parts of date to transform\n",
    "        date_log = '-'.join(date_part)\n",
    "        # remove the '.log' extension and pass the number to int\n",
    "        number_log = int(number_part.split('.')[0])\n",
    "        # return the date and number for sorting\n",
    "        return date_log, number_log\n",
    "\n",
    "    # use the logs_modifications function to sort the logs by date and number\n",
    "    sorted_logs = sorted(logs, key=logs_modifications)\n",
    "    return sorted_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folder creation if not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir(server_app_folder)\n",
    "mkdir(raw_logs_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Pterodactyl Application information\n",
    "About: locations, nodes, nests, eggs, servers, clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data from Pterodactyl App\n",
    "list_of_clients = api_app.user.list_users()\n",
    "all_clients = [[client['attributes']for client in clients]for clients in list_of_clients]\n",
    "\n",
    "list_of_locations = api_app.locations.list_locations()\n",
    "all_locations = [[location['attributes']for location in locations]for locations in list_of_locations]\n",
    "\n",
    "list_of_nodes = api_app.nodes.list_nodes()\n",
    "all_nodes = [[node['attributes']for node in nodes]for nodes in list_of_nodes]\n",
    "\n",
    "list_of_nests_and_eggs = api_app.nests.list_nests(includes=['eggs'])\n",
    "all_nests = [[nest['attributes'] for nest in nests] for nests in list_of_nests_and_eggs]\n",
    "all_eggs = [[eggs['attributes'] for eggs in nests['attributes']['relationships']['eggs']['data']] for nests in list_of_nests_and_eggs]\n",
    "\n",
    "list_of_servers_and_clients = api_app.servers.list_servers(includes=['subusers'])\n",
    "all_servers = [[server['attributes']for server in servers] for servers in list_of_servers_and_clients]\n",
    "all_client_server = [[client_server['attributes'] for client_server in servers['attributes']['relationships']['subusers']['data']] for servers in list_of_servers_and_clients]\n",
    "\n",
    "# Get the current timestamp with timezone information (UTC)\n",
    "last_update = datetime.datetime.now(datetime.timezone.utc)\n",
    "\n",
    "# Cleaning and filtering columns\n",
    "df_clients = pd.DataFrame(all_clients[0])\n",
    "df_clients = df_clients[['id', 'uuid', 'username', 'email', 'first_name', 'last_name', 'root_admin', '2fa', 'created_at', 'updated_at']].rename(columns={'username': 'client_name', 'root_admin': 'admin'})\n",
    "\n",
    "df_locations = pd.DataFrame(all_locations[0])\n",
    "df_locations = df_locations[['id', 'short', 'long', 'created_at', 'updated_at']]\n",
    "\n",
    "df_nodes = pd.DataFrame(all_nodes[0])\n",
    "df_nodes['allocated_memory'] = df_nodes['allocated_resources'].apply(lambda x: x.get('memory', None))\n",
    "df_nodes['allocated_disk'] = df_nodes['allocated_resources'].apply(lambda x: x.get('disk', None))\n",
    "df_nodes = df_nodes[['id', 'uuid', 'public', 'name', 'description', 'location_id', 'fqdn', 'scheme', 'behind_proxy', 'maintenance_mode', 'memory', 'disk', 'allocated_memory', 'allocated_disk', 'upload_size', 'daemon_listen', 'daemon_sftp', 'daemon_base','created_at', 'updated_at']].rename(columns={'': '', '': ''})\n",
    "\n",
    "df_nests = pd.DataFrame(all_nests[0])\n",
    "df_nests = df_nests[['id', 'uuid', 'name', 'description', 'author', 'created_at', 'updated_at']]\n",
    "\n",
    "df_eggs = pd.DataFrame(all_eggs[0])\n",
    "df_eggs = df_eggs[['id', 'uuid', 'name', 'description', 'nest', 'author', 'created_at', 'updated_at']].rename(columns={'nest': 'nest_id'})\n",
    "\n",
    "df_servers = pd.DataFrame(all_servers[0])\n",
    "df_servers['limit_memory'] = df_servers['limits'].apply(lambda x: x.get('memory', None))\n",
    "df_servers['limit_disk'] = df_servers['limits'].apply(lambda x: x.get('disk', None))\n",
    "df_servers['limit_io'] = df_servers['limits'].apply(lambda x: x.get('io', None))\n",
    "df_servers['limit_cpu'] = df_servers['limits'].apply(lambda x: x.get('cpu', None))\n",
    "df_servers['limit_oom_disable'] = df_servers['limits'].apply(lambda x: x.get('oom_disable', None))\n",
    "df_servers['limit_database'] = df_servers['feature_limits'].apply(lambda x: x.get('database', None))\n",
    "df_servers['limit_allocation'] = df_servers['feature_limits'].apply(lambda x: x.get('allocation', None))\n",
    "df_servers['limit_backup'] = df_servers['feature_limits'].apply(lambda x: x.get('backup', None))\n",
    "df_servers = df_servers[['id', 'uuid', 'identifier', 'name', 'description', 'limit_memory', 'limit_disk', 'limit_io', 'limit_cpu', 'limit_oom_disable', 'limit_database', 'limit_allocation', 'limit_backup', 'user', 'node', 'allocation', 'nest', 'egg','created_at', 'updated_at']].rename(columns={'user': 'client_id', 'node': 'node_id', 'allocation': 'allocation_id', 'nest': 'nest_id', 'egg': 'egg_id'})\n",
    "\n",
    "flattened_client_server = [item for sublist in all_client_server for item in sublist]\n",
    "df_clients_server = pd.DataFrame(flattened_client_server)[['id', 'user_id', 'server_id', 'created_at', 'updated_at']].rename(columns={'user_id': 'client_id'})\n",
    "df_clients_server\n",
    "\n",
    "# Exporting data into .csv files\n",
    "df_clients.to_csv(os.path.join(server_app_folder, 'clients.csv'), index=False)\n",
    "df_locations.to_csv(os.path.join(server_app_folder,'locations.csv'), index=False)\n",
    "df_nodes.to_csv(os.path.join(server_app_folder,'nodes.csv'), index=False)\n",
    "df_nests.to_csv(os.path.join(server_app_folder,'nests.csv'), index=False)\n",
    "df_eggs.to_csv(os.path.join(server_app_folder,'eggs.csv'), index=False)\n",
    "df_servers.to_csv(os.path.join(server_app_folder,'servers.csv'), index=False)\n",
    "df_clients_server.to_csv(os.path.join(server_app_folder,'clients_server.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload csv table files into Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(connection)\n",
    "\n",
    "for file_table in os.listdir(server_app_folder):\n",
    "\n",
    "    # Setup database variables\n",
    "    ID = 'id'\n",
    "    TABLE = file_table.split('.')[0]\n",
    "    TABLE_UPDATE = TABLE + '_update'\n",
    "    IS_ACTIVE_TABLE = 'is_active_table'\n",
    "    SCHEMA = 'pterodactyl'\n",
    "    \n",
    "    # Reading of the file_table\n",
    "    df = pd.read_csv(os.path.join(server_app_folder, file_table))\n",
    "\n",
    "    # Start connection with database\n",
    "    with engine.connect() as conn:\n",
    "        # Start a new transaction\n",
    "        trans = conn.begin()\n",
    "\n",
    "        try:\n",
    "            # Load ID from database\n",
    "            result = conn.execute(text(f'SELECT \"{ID}\" FROM {SCHEMA}.{TABLE}'))\n",
    "            db = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "\n",
    "            # Compare ID\n",
    "            sameID = db[ID].isin(df[ID])\n",
    "\n",
    "            toUpdate = df[df[ID].isin(db[ID][sameID])]\n",
    "            toIngest = df[~df[ID].isin(db[ID][sameID])]\n",
    "            toDelete = db[~db[ID].isin(df[ID])]\n",
    "\n",
    "            # Insert the DataFrame into a table\n",
    "            toIngest.to_sql(TABLE, conn, schema=SCHEMA, if_exists='append', index=False)\n",
    "\n",
    "            # Insert the updatable DataFrame into the TABLE_UPDATE table\n",
    "            toUpdate.to_sql(TABLE_UPDATE, conn, schema=SCHEMA, if_exists='append', index=False)\n",
    "\n",
    "            # Define and execute the following queries\n",
    "            conn.execute(text(f'DELETE FROM {SCHEMA}.{TABLE} WHERE \"{ID}\" IN (SELECT \"{ID}\" FROM {SCHEMA}.{TABLE_UPDATE});'))\n",
    "            conn.execute(text(f'INSERT INTO {SCHEMA}.{TABLE} SELECT * FROM {SCHEMA}.{TABLE_UPDATE};'))\n",
    "            conn.execute(text(f'TRUNCATE TABLE {SCHEMA}.{TABLE_UPDATE};'))\n",
    "\n",
    "            # Update column \"is_active\" from tables when data is deleted from Pterodactyl App\n",
    "            toDelete.to_sql(IS_ACTIVE_TABLE, conn, schema=SCHEMA, if_exists='append', index=False)\n",
    "            conn.execute(text(f'UPDATE {SCHEMA}.{TABLE} SET is_active = false WHERE \"{ID}\" IN (SELECT * FROM {SCHEMA}.{IS_ACTIVE_TABLE});'))\n",
    "            conn.execute(text(f'TRUNCATE TABLE {SCHEMA}.{IS_ACTIVE_TABLE};'))\n",
    "\n",
    "            # Commit the transaction\n",
    "            trans.commit()\n",
    "\n",
    "        except Exception as e:\n",
    "            # Rollback the transaction on exception\n",
    "            print('!!! [ERROR IN DATABASE QUERIES] !!!')\n",
    "            trans.rollback()\n",
    "            print('Transaction has been rolled back')\n",
    "            print(f'Error occurred during transaction:\\n{e}')\n",
    "            raise\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Start a new transaction\n",
    "    trans = conn.begin()\n",
    "\n",
    "    try:\n",
    "        # Update date from the last_update table based on max date on the file\n",
    "        old_last_update = conn.execute(text(f'SELECT date FROM {SCHEMA}.last_update')).fetchall()[0][0]\n",
    "        new_last_update = last_update\n",
    "        if new_last_update > old_last_update:\n",
    "            conn.execute(text(f\"UPDATE {SCHEMA}.last_update SET date = '{new_last_update}';\"))\n",
    "\n",
    "        # Commit the transaction\n",
    "        trans.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        # Rollback the transaction on exception\n",
    "        print('!!! [ERROR IN DATABASE QUERIES] !!!')\n",
    "        trans.rollback()\n",
    "        print('Transaction has been rolled back')\n",
    "        print(f'Error occurred during transaction:\\n{e}')\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract logs from each active Minecraft Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pattern = r'^\\d{4}-\\d{2}-\\d{2}-\\d.*' # yyyy-mm-dd-n*\n",
    "extension_file_compressed = '.log.gz'\n",
    "extension_file_uncompessed = '.log'\n",
    "output_logs_folder = os.path.join(pwd, 'processed_logs')\n",
    "SCHEMA = 'pterodactyl'\n",
    "\n",
    "eggs_ready = ['Vanilla Minecraft', 'Forge Minecraft', 'Paper'] # Vanilla Bedrock is still not ready to be processed\n",
    "\n",
    "# Create folder output_logs_folder\n",
    "mkdir(output_logs_folder)\n",
    "\n",
    "# Get server information\n",
    "engine = create_engine(connection)\n",
    "with engine.connect() as conn:\n",
    "    list_servers = conn.execute(text(f'SELECT servers.identifier, eggs.name FROM {SCHEMA}.servers JOIN {SCHEMA}.eggs ON eggs.id = servers.egg_id WHERE servers.is_active = true and servers.nest_id = 1'))\n",
    "\n",
    "for server_info in list_servers:\n",
    "\n",
    "    if server_info[1] in eggs_ready:\n",
    "        # Create a folder\n",
    "        folder_name = server_info[0]\n",
    "        folder_server_dir = os.path.join(raw_logs_folder, folder_name)\n",
    "        mkdir(folder_server_dir)\n",
    "\n",
    "        # Create a sub-folder called last_log inside of the folder of the server\n",
    "        last_log_dir = os.path.join(folder_server_dir, 'last_log')\n",
    "        mkdir(last_log_dir)\n",
    "\n",
    "        # Try to get userchache.json file from server\n",
    "        try:\n",
    "            # Download users in cache\n",
    "            users_cache = api_cli.client.servers.files.get_file_contents(server_info[0], 'usercache.json')\n",
    "            user_names = [user['name'] for user in users_cache]\n",
    "        except:\n",
    "            user_names = [None]\n",
    "\n",
    "        # Add new users only\n",
    "        with open(os.path.join(raw_logs_folder, folder_name, 'users.csv'), 'a+', newline='') as csvfile:\n",
    "            csvfile.seek(0)\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            existing_names = [row['name'] for row in reader]\n",
    "            new_names = [name for name in user_names if name not in existing_names]\n",
    "\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=['name'])\n",
    "            if not existing_names:\n",
    "                writer.writeheader()\n",
    "\n",
    "            for name in new_names:\n",
    "                writer.writerow({'name': name})\n",
    "\n",
    "        # Download new log files\n",
    "        # Select the last log name in folder last_log\n",
    "        if os.listdir(last_log_dir):\n",
    "            last_log_name = os.listdir(last_log_dir)[0]\n",
    "        else:\n",
    "            last_log_name = None\n",
    "        # Get a sorted list of the logs inside the server\n",
    "        log_files = api_cli.client.servers.files.list_files(server_info[0], '/logs/')\n",
    "        list_logs = [file['attributes']['name'] for file in log_files['data'] if re.match(log_pattern, file['attributes']['name'])]\n",
    "        sorted_list_logs = sort_list_logs(list_logs)\n",
    "        # Select only a list of downloadable_logs which are new; after the last_log_name\n",
    "        try:\n",
    "            index_last_log = sorted_list_logs.index(last_log_name + '.gz')\n",
    "        except:\n",
    "            index_last_log = -1\n",
    "        downloadable_logs = sorted_list_logs[index_last_log + 1:]\n",
    "        # Download all logs in the list downloadable_logs\n",
    "        list_download = [api_cli.client.servers.files.download_file(server_info[0], f'/logs/{log}') for log in downloadable_logs]\n",
    "        if list_download:\n",
    "            [urllib.request.urlretrieve(list_download[i], os.path.join(folder_server_dir, list_logs[i])) for i in range(len(list_download))]\n",
    "        print(f'Files downloaded: {len(list_download)}')\n",
    "\n",
    "        # Uncompressing files\n",
    "        for filename in os.listdir(folder_server_dir):\n",
    "            if filename.endswith(extension_file_compressed):\n",
    "                compressed_file_path = os.path.join(folder_server_dir,filename)\n",
    "                decompressed_file_path = os.path.splitext(compressed_file_path)[0] # Remove the .gz extension\n",
    "\n",
    "                # Uncompress the file\n",
    "                with gzip.open(compressed_file_path, 'rb') as compressed_file:\n",
    "                    with open(decompressed_file_path, 'wb') as decompressed_file:\n",
    "                        decompressed_file.write(compressed_file.read())\n",
    "\n",
    "                # Delete the compressed file\n",
    "                os.remove(compressed_file_path)\n",
    "\n",
    "        # If there are .log files in folder_server_dir\n",
    "        if [file for file in os.listdir(folder_server_dir) if re.match(log_pattern, file)]:\n",
    "            # Delete last log file from folder last_log_dir\n",
    "            if os.listdir(last_log_dir):\n",
    "                os.remove(os.path.join(last_log_dir, os.listdir(last_log_dir)[0]))\n",
    "\n",
    "            # Create a empty file called as the last_log in folder last_log\n",
    "            list_local_logs = sort_list_logs([file for file in os.listdir(folder_server_dir) if re.match(log_pattern, file)])\n",
    "            with open(os.path.join(last_log_dir, list_local_logs[-1]), 'w'):\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation from logs data to information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_logs = pd.DataFrame(columns=['server_id', 'date', 'time', 'information', 'user', 'activity'])\n",
    "\n",
    "for folder in os.listdir(raw_logs_folder): \n",
    "    folder_server_dir = os.path.join(raw_logs_folder, folder)\n",
    "\n",
    "    # Read all logs as one\n",
    "    log_files = [log for log in os.listdir(folder_server_dir) if log.endswith(extension_file_uncompessed)]\n",
    "    log_files = sort_list_logs(log_files)\n",
    "\n",
    "    if log_files:\n",
    "\n",
    "        all_logs = \"\"\n",
    "        for log_file in log_files:\n",
    "            with open(os.path.join(folder_server_dir, log_file), 'r', encoding='utf-8') as file:\n",
    "                log_contents = file.read().split('\\n')\n",
    "                # This wont work with 2 digits (get the date from the file name)\n",
    "                log_contents = \"\\n\".join([f'[{log_file[:-(2+len(extension_file_uncompessed))]}] ' + line for line in log_contents if line.strip() != \"\"])\n",
    "                all_logs += log_contents + \"\\n\"\n",
    "\n",
    "        # Transform information it in meaningful information\n",
    "        pattern = r'\\[(\\d{4}-\\d{2}-\\d{2})\\] \\[.*?(\\d{2}:\\d{2}:\\d{2}).*?\\] \\[(.*?)/.*?\\]: (.*?)\\n'\n",
    "        matches = re.findall(pattern, all_logs)\n",
    "\n",
    "        # Create a list of dictionaries to store the extracted data\n",
    "        log_data = [{'server_id': folder, 'date': match[0], 'time': match[1], 'category': match[2], 'information': match[3]} for match in matches]\n",
    "        # Create a dataframe of the logs\n",
    "        df_logs = pd.DataFrame(log_data)\n",
    "        df_logs['user'] = None\n",
    "        df_logs['activity'] = None\n",
    "\n",
    "        # Filter by column category selecting only Server thread logs nad delete that column\n",
    "        df_logs = df_logs[df_logs['category'] == 'Server thread'][['server_id', 'date', 'time', 'information', 'user', 'activity']]\n",
    "\n",
    "        # Create a dataframe of the users\n",
    "        df_users = pd.read_csv(os.path.join(folder_server_dir,'users.csv'))\n",
    "\n",
    "        # Add information when server started\n",
    "        index=df_logs[df_logs['information'].str.startswith(\"Starting minecraft server version\")].index\n",
    "        if not index.empty:\n",
    "            df_logs.loc[index, 'user'] = 'server'\n",
    "            df_logs.loc[index, 'activity'] = 'start'\n",
    "        # Add information when server stopped\n",
    "        index=df_logs[df_logs['information'].str.startswith(\"Stopping server\")].index\n",
    "        if not index.empty:\n",
    "            df_logs.loc[index, 'user'] = 'server'\n",
    "            df_logs.loc[index, 'activity'] = 'stop'\n",
    "        # Add information when user did something\n",
    "        for user in df_users['name']:\n",
    "            # Add information when user activity\n",
    "            index=df_logs[df_logs['information'].str.startswith(user)].index\n",
    "            if not index.empty:\n",
    "                df_logs.loc[index, 'user'] = user\n",
    "                df_logs.loc[index, 'activity'] = 'action'\n",
    "            # Add information when user login\n",
    "            index=df_logs[df_logs['information'].str.startswith(f'{user} joined the game')].index\n",
    "            if not index.empty:\n",
    "                df_logs.loc[index, 'user'] = user\n",
    "                df_logs.loc[index, 'activity'] = 'login'\n",
    "            # Add information when user logout\n",
    "            index=df_logs[df_logs['information'].str.startswith(f'{user} left the game')].index\n",
    "            if not index.empty:\n",
    "                df_logs.loc[index, 'user'] = user\n",
    "                df_logs.loc[index, 'activity'] = 'logout'\n",
    "            # Delete duplicated activity of login and logout\n",
    "            index=df_logs[df_logs['information'].str.startswith(f'{user}[') | df_logs['information'].str.startswith(f'{user} lost connection: Disconnected')].index\n",
    "            if not index.empty:\n",
    "                df_logs.loc[index, 'user'] = None\n",
    "                df_logs.loc[index, 'activity'] = None\n",
    "        # Rows with no server/user activity is deleted\n",
    "        df_logs = df_logs.dropna(subset=['activity'])\n",
    "\n",
    "        # Save every log in df_all_logs for each iteration\n",
    "        df_all_logs = pd.concat([df_all_logs, df_logs], ignore_index=True)\n",
    "\n",
    "df_all_logs.to_parquet(os.path.join(output_logs_folder, 'activity.parquet'), index=False)\n",
    "\n",
    "# Delete all logs inside each server folder\n",
    "for folder in os.listdir(raw_logs_folder):\n",
    "    folder_server_dir = os.path.join(raw_logs_folder, folder)\n",
    "    [os.remove(os.path.join(folder_server_dir, log)) for log in os.listdir(folder_server_dir) if log.endswith(extension_file_uncompessed)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processed data into Data Warehouse (Postgres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parquet file into a dataframe\n",
    "all_new_activity=pd.read_parquet(os.path.join(output_logs_folder, 'activity.parquet'))\n",
    "\n",
    "# Connect to database and upload all new logs into table\n",
    "engine = create_engine(connection)\n",
    "with engine.connect() as conn:\n",
    "\n",
    "# Start a new transaction\n",
    "    trans = conn.begin()\n",
    "\n",
    "    try:\n",
    "        # Load all new activity into postgres\n",
    "        all_new_activity.to_sql(name = 'activity', schema = SCHEMA, con = conn, if_exists='append', index=False)\n",
    "        # Commit the transaction\n",
    "        trans.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        # Rollback the transaction on exception\n",
    "        print('!!! [ERROR IN DATABASE QUERIES] !!!')\n",
    "        trans.rollback()\n",
    "        print('Transaction has been rolled back')\n",
    "        print(f'Error occurred during transaction:\\n{e}')\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
