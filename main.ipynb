{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline for Pterodactyl Minecraft Servers Analysis\n",
    "\n",
    "## Index\n",
    "\n",
    "- Install requierements\n",
    "- Import libraries and setup key variables\n",
    "- Extract data from source\n",
    "- Transformation from data to information\n",
    "- Load processed data into Data Warehouse (Postgres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install requierements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and setup key variables\n",
    "\n",
    "Remember to add you own credentials in the .env file for them to be loaded here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, psycopg2, pyarrow, json, csv, gzip, re, os\n",
    "from sqlalchemy import create_engine, text\n",
    "from pydactyl import PterodactylClient\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# Load .env file credentials\n",
    "load_dotenv()\n",
    "\n",
    "# Database connection\n",
    "host = os.getenv('POSTGRES_HOST')\n",
    "port = os.getenv('POSTGRES_PORT')\n",
    "dbname = os.getenv('POSTGRES_DBNAME')\n",
    "user = os.getenv('POSTGRES_USER')\n",
    "password = os.getenv('POSTGRES_PASSWORD')\n",
    "connection = f'postgresql://{user}:{password}@{host}:{port}/{dbname}'\n",
    "\n",
    "# Pterodactyl connection\n",
    "pterodactyl_url = os.getenv('PTERODACTYL_URL')\n",
    "client_api_key = os.getenv('PTERODACTYL_API_KEY')\n",
    "\n",
    "# Connecto to Pterodactyl API and get list of servers\n",
    "api = PterodactylClient(pterodactyl_url, client_api_key, debug=False)\n",
    "servers_object = api.client.servers.list_servers(includes=['egg'])\n",
    "list_servers = [[server['attributes'] for server in servers] for servers in servers_object][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data from source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['identifier', 'name', 'node']\n",
    "limits_keys = ['memory', 'disk']\n",
    "log_pattern = r'^\\d{4}-\\d{2}-\\d{2}-\\d.*' # yyyy-mm-dd-n.log.gz\n",
    "extension_file_compressed = '.log.gz'\n",
    "extension_file_uncompessed = '.log'\n",
    "pwd = os.getcwd() #os.path.dirname(os.path.realpath(__file__)) this is used for .py files\n",
    "raw_data_folder = 'raw_data'\n",
    "output_folder = os.path.join(pwd, 'output')\n",
    "\n",
    "eggs_ready = ['Vanilla Minecraft', 'Forge Minecraft', 'Paper'] # Vanilla Bedrock is still not ready to be processed\n",
    "\n",
    "for server in list_servers:\n",
    "    # Get server information\n",
    "    server_data = {key: server[key] for key in keys}\n",
    "    server_resources = {key: server['limits'][key] for key in limits_keys}\n",
    "    server_egg = {'egg': server['relationships']['egg']['attributes']['name']}\n",
    "    server_docker = {'docker_image': server['docker_image']}\n",
    "    server_info = {**server_data, **server_egg, **server_docker, **server_resources}\n",
    "\n",
    "    if server_info['egg'] in eggs_ready:\n",
    "        # Create a folder\n",
    "        folder_name = server_info['identifier']\n",
    "        path_folder = os.path.join(pwd, raw_data_folder, folder_name)\n",
    "        if not os.path.exists(path_folder):\n",
    "            os.makedirs(path_folder)\n",
    "\n",
    "        # Download Metadata\n",
    "        with open(os.path.join(path_folder, 'metadata.json'), \"w\") as json_file:\n",
    "            json.dump([server_info], json_file, indent=4)\n",
    "\n",
    "        # Download users in cache\n",
    "        fieldnames = ['name']\n",
    "        users_cache = api.client.servers.files.get_file_contents(server_info['identifier'], 'usercache.json')\n",
    "        user_names = [user['name'] for user in users_cache]\n",
    "\n",
    "        # Add new users only\n",
    "        file_exists = os.path.exists(os.path.join(path_folder, 'users.csv'))\n",
    "        existing_names = []\n",
    "        with open(os.path.join(path_folder, 'users.csv'), 'a+', newline='') as csvfile:\n",
    "            csvfile.seek(0) \n",
    "            reader = csv.DictReader(csvfile)\n",
    "            existing_names = [row['name'] for row in reader]\n",
    "            new_names = [name for name in user_names if name not in existing_names]\n",
    "\n",
    "            if not file_exists:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            for name in new_names:\n",
    "                writer.writerow({'name': name})\n",
    "\n",
    "        # Download new log files\n",
    "        local_logs = [filename for filename in os.listdir(path_folder) if re.match(log_pattern, filename)]\n",
    "        log_files = api.client.servers.files.list_files(server_info['identifier'], '/logs/')\n",
    "        list_logs = [file['attributes']['name'] for file in log_files['data'] if re.match(log_pattern, file['attributes']['name']) and file['attributes']['name'][0:-(len(extension_file_compressed)-len(extension_file_uncompessed))] not in local_logs]\n",
    "        list_download = [api.client.servers.files.download_file(server_info['identifier'], f'/logs/{log}') for log in list_logs]\n",
    "        [urllib.request.urlretrieve(list_download[i], os.path.join(path_folder,list_logs[i])) for i in range(len(list_logs))]\n",
    "        print(f'Files downloaded: {len(list_download)}')\n",
    "\n",
    "        # Uncompressing files\n",
    "        for filename in os.listdir(path_folder):\n",
    "            if filename.endswith(extension_file_compressed):\n",
    "                compressed_file_path = os.path.join(path_folder,filename)\n",
    "                decompressed_file_path = os.path.splitext(compressed_file_path)[0] # Remove the .gz extension\n",
    "\n",
    "                # Uncompress the file\n",
    "                with gzip.open(compressed_file_path, 'rb') as compressed_file:\n",
    "                    with open(decompressed_file_path, 'wb') as decompressed_file:\n",
    "                        decompressed_file.write(compressed_file.read())\n",
    "\n",
    "                # Delete the compressed file\n",
    "                os.remove(compressed_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation from data to information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_logs = pd.DataFrame(columns=['server_id', 'date', 'time', 'information', 'user', 'activity'])\n",
    "df_all_server_info = pd.DataFrame(columns=['identifier', 'name', 'node', 'memory', 'disk'])\n",
    "\n",
    "for folder in os.listdir(os.path.join(pwd, raw_data_folder)): \n",
    "    path_folder = os.path.join(pwd, raw_data_folder, folder)\n",
    "\n",
    "    # Read all logs as one\n",
    "    log_files = [log for log in os.listdir(path_folder) if log.endswith(extension_file_uncompessed)]\n",
    "    log_files.sort()\n",
    "\n",
    "    all_logs = \"\"\n",
    "    for log_file in log_files:\n",
    "        with open(os.path.join(path_folder, log_file), 'r') as file:\n",
    "            log_contents = file.read().split('\\n')\n",
    "            log_contents = \"\\n\".join([f'[{log_file[:-(2+len(extension_file_uncompessed))]}] ' + line for line in log_contents if line.strip() != \"\"])\n",
    "            all_logs += log_contents + \"\\n\"\n",
    "\n",
    "    # Transform information it in meaningful information\n",
    "    pattern = r'\\[(\\d{4}-\\d{2}-\\d{2})\\] \\[.*?(\\d{2}:\\d{2}:\\d{2}).*?\\] \\[(.*?)/.*?\\]: (.*?)\\n'\n",
    "    matches = re.findall(pattern, all_logs)\n",
    "    \n",
    "    # Create a list of dictionaries to store the extracted data\n",
    "    log_data = [{'server_id': folder, 'date': match[0], 'time': match[1], 'category': match[2], 'information': match[3]} for match in matches]\n",
    "    # Create a dataframe of the logs\n",
    "    df_logs = pd.DataFrame(log_data)\n",
    "    # Filter by column category selecting only Server thread logs nad delete that column\n",
    "    df_logs = df_logs[df_logs['category'] == 'Server thread'][['server_id', 'date', 'time', 'information']]\n",
    "\n",
    "    # Create a dataframe of the users\n",
    "    df_users = pd.read_csv(os.path.join(path_folder,'users.csv'))\n",
    "\n",
    "    # Add information when server started\n",
    "    index=df_logs[df_logs['information'].str.startswith(\"Starting minecraft server version\")].index\n",
    "    df_logs.loc[index, 'user'] = 'server'\n",
    "    df_logs.loc[index, 'activity'] = 'start'\n",
    "    # Add information when server stopped\n",
    "    index=df_logs[df_logs['information'].str.startswith(\"Stopping server\")].index\n",
    "    df_logs.loc[index, 'user'] = 'server'\n",
    "    df_logs.loc[index, 'activity'] = 'stop'\n",
    "    # Add information when user did something\n",
    "    for user in df_users['name']:\n",
    "        # Add information when user activity\n",
    "        index=df_logs[df_logs['information'].str.startswith(user)].index\n",
    "        df_logs.loc[index, 'user'] = user\n",
    "        df_logs.loc[index, 'activity'] = 'action'\n",
    "        # Add information when user login\n",
    "        index=df_logs[df_logs['information'].str.startswith(f'{user} joined the game')].index\n",
    "        df_logs.loc[index, 'user'] = user\n",
    "        df_logs.loc[index, 'activity'] = 'login'\n",
    "        # Add information when user logout\n",
    "        index=df_logs[df_logs['information'].str.startswith(f'{user} left the game')].index\n",
    "        df_logs.loc[index, 'user'] = user\n",
    "        df_logs.loc[index, 'activity'] = 'logout'\n",
    "        # Delete duplicated activity of login and logout\n",
    "        index=df_logs[df_logs['information'].str.startswith(f'{user}[') | df_logs['information'].str.startswith(f'{user} lost connection: Disconnected')].index\n",
    "        df_logs.loc[index, 'user'] = None\n",
    "        df_logs.loc[index, 'activity'] = None\n",
    "    # Rows with no server/user activity is deleted\n",
    "    df_logs = df_logs.dropna(subset=['activity'])\n",
    "\n",
    "    # Read JSON file with metadata of the server\n",
    "    df_server_info = pd.read_json(os.path.join(path_folder, 'metadata.json'), orient='records')\n",
    "\n",
    "    # Save every log in df_all_logs for each iteration\n",
    "    df_all_logs = pd.concat([df_all_logs, df_logs], ignore_index=True)\n",
    "    df_all_server_info = pd.concat([df_all_server_info, df_server_info], ignore_index=True)\n",
    "\n",
    "# Export it in parquet file\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "df_all_logs.to_parquet(os.path.join(output_folder, 'fact_logs.parquet'), index=False)\n",
    "df_all_server_info.to_parquet(os.path.join(output_folder, 'dim_server.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processed data into Data Warehouse (Postgres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually load only fact_logs into the emtpy table\n",
    "\n",
    "pwd = os.getcwd() #os.path.dirname(os.path.realpath(__file__))\n",
    "output_folder = os.path.join(pwd, 'output')\n",
    "\n",
    "df=pd.read_parquet(os.path.join(output_folder, 'fact_logs.parquet'))\n",
    "engine = create_engine(connection)\n",
    "with engine.connect() as conn:\n",
    "    df.to_sql(name = 'activity', schema = 'pterodactyl', con = conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually load only dim_server into the emtpy table\n",
    "\n",
    "pwd = os.getcwd() #os.path.dirname(os.path.realpath(__file__))\n",
    "output_folder = os.path.join(pwd, 'output')\n",
    "\n",
    "df=pd.read_parquet(os.path.join(output_folder, 'dim_server.parquet'))\n",
    "engine = create_engine(connection)\n",
    "with engine.connect() as conn:\n",
    "    df.to_sql(name = 'servers', schema = 'pterodactyl', con = conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS ONLY A TEMPLATE FROM ANOTHER PROJECT, PLEASE MODIFY AS NEEDED\n",
    "\n",
    "#for parquet in os.listdir(output_folder):\n",
    "\n",
    "engine = create_engine(connection)\n",
    "\n",
    "# Start connection with database\n",
    "with engine.connect() as conn:\n",
    "    # Start a new transaction\n",
    "    trans = conn.begin()\n",
    "\n",
    "    try:\n",
    "        # Load ID from database\n",
    "        print('  - DataBase Loading')\n",
    "        result = conn.execute(text(f'SELECT \"{ID}\" FROM {TABLE}'))\n",
    "        dataBase = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "\n",
    "        # Compare ID\n",
    "        print('  - Data Comparison')\n",
    "        sameID = dataBase[ID].isin(dataframe[ID])\n",
    "\n",
    "        toUpdate = dataframe[dataframe[ID].isin(dataBase[ID][sameID])]\n",
    "        toIngest = dataframe[~dataframe[ID].isin(dataBase[ID][sameID])]\n",
    "\n",
    "        # Insert the DataFrame into a table\n",
    "        print('  - Insert new data to Table')\n",
    "        toIngest.to_sql(TABLE, conn, if_exists='append', index=False)\n",
    "        print(f'    - Rows inserted: {toIngest.shape[0]}')\n",
    "\n",
    "        # Insert the updatable DataFrame into the TABLE_UPDATE table\n",
    "        print('  - Insert updated data to Auxiliary Table')\n",
    "        toUpdate.to_sql(TABLE_UPDATE, conn, if_exists='append', index=False)\n",
    "        print(f'    - Rows inserted: {toUpdate.shape[0]}')\n",
    "\n",
    "        # Define and execute the following queries\n",
    "        print('  - Updating data from Auxiliry Table to Table')\n",
    "        print('    - Delete rows to be updated in Table')\n",
    "        conn.execute(text(f'DELETE FROM {TABLE} WHERE \"{ID}\" IN (SELECT \"{ID}\" FROM {TABLE_UPDATE});'))\n",
    "        print('    - Insert updated rows from Auxiliary Table to Table')\n",
    "        conn.execute(text(f'INSERT INTO {TABLE} SELECT * FROM {TABLE_UPDATE};'))\n",
    "        print('    - Truncate Auxiliary Table')\n",
    "        conn.execute(text(f'TRUNCATE TABLE {TABLE_UPDATE};'))\n",
    "\n",
    "        # Update date from the last_update table based on max date on the file\n",
    "        print('  - Update date from last_update table')\n",
    "        old_last_update = conn.execute(text(f'SELECT \"{TABLE}\" FROM last_update')).fetchall()[0][0]\n",
    "        new_last_update = date_file\n",
    "        if new_last_update > old_last_update:\n",
    "            conn.execute(text(f\"UPDATE last_update SET {TABLE} = '{new_last_update}' WHERE {TABLE} = '{old_last_update}';\"))\n",
    "\n",
    "        # Commit the transaction\n",
    "        trans.commit()\n",
    "        print('- Transaction commited')\n",
    "        print('- Disconnecting from the database\\n')\n",
    "\n",
    "    except Exception as e:\n",
    "        # Rollback the transaction on exception\n",
    "        print('!!! [ERROR IN DATABASE QUERIES] !!!')\n",
    "        trans.rollback()\n",
    "        print('Transaction has been rolled back')\n",
    "        print(f'Error occurred during transaction:\\n{e}')\n",
    "        raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
