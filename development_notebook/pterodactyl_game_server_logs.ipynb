{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline for Pterodactyl Game Servers Logs\n",
    "\n",
    "### Index\n",
    "\n",
    "- Install requierements\n",
    "- Import libraries and setup key variables\n",
    "- Setup directories, functions and folder creation\n",
    "- Extract logs from each active Minecraft Server\n",
    "- Transformation from logs data into information\n",
    "- Load processed data into Data Warehouse (Postgres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install requierements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and setup key variables\n",
    "Remember to add you own credentials in the .env file for them to be loaded here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, zipfile, requests, gzip, csv, os, re\n",
    "from sqlalchemy import create_engine, text\n",
    "from pydactyl import PterodactylClient\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# Load .env file credentials\n",
    "load_dotenv()\n",
    "\n",
    "# Database connection\n",
    "host = os.getenv('POSTGRES_HOST')\n",
    "port = os.getenv('POSTGRES_PORT')\n",
    "database = os.getenv('POSTGRES_DATABASE')\n",
    "username = os.getenv('POSTGRES_USERNAME')\n",
    "password = os.getenv('POSTGRES_PASSWORD')\n",
    "connection = f'postgresql://{username}:{password}@{host}:{port}/{database}'\n",
    "\n",
    "# Pterodactyl connection\n",
    "pterodactyl_url = os.getenv('PTERODACTYL_URL')\n",
    "application_api_key = os.getenv('PTERODACTYL_APP_KEY')\n",
    "client_api_key = os.getenv('PTERODACTYL_CLI_KEY')\n",
    "\n",
    "# Connecto to Pterodactyl Application API\n",
    "#api_app = PterodactylClient(pterodactyl_url, application_api_key, debug=False)\n",
    "# Connecto to Pterodactyl Client API\n",
    "#api_cli = PterodactylClient(pterodactyl_url, client_api_key, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup directories, functions and folder creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directories\n",
    "pwd = os.getcwd() #os.path.dirname(os.path.realpath(__file__)) this is used for .py files\n",
    "raw_logs_folder = os.path.join(pwd, 'raw_logs')\n",
    "\n",
    "# Create new folder if not exists\n",
    "def mkdir(folder_dir):\n",
    "    if not os.path.exists(folder_dir):\n",
    "        os.makedirs(os.path.join(pwd, folder_dir))\n",
    "\n",
    "# Sort a list of logs names\n",
    "def sort_list_logs(logs):\n",
    "    def logs_modifications(log):\n",
    "        # remove the '-' from the log\n",
    "        date_part, number_part = log.split('-')[:3], log.split('-')[3]\n",
    "        # Join the parts of date to transform\n",
    "        date_log = '-'.join(date_part)\n",
    "        # remove the '.log' extension and pass the number to int\n",
    "        number_log = int(number_part.split('.')[0])\n",
    "        # return the date and number for sorting\n",
    "        return date_log, number_log\n",
    "\n",
    "    # use the logs_modifications function to sort the logs by date and number\n",
    "    sorted_logs = sorted(logs, key=logs_modifications)\n",
    "    return sorted_logs\n",
    "\n",
    "# Get last index from a list when matching with an element\n",
    "def last_index(list, element):\n",
    "    try:\n",
    "        return len(list) - 1 - list[::-1].index(element)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"'{element}' not found in the list\")\n",
    "\n",
    "# Extract the compressed files (.gz and .zip)    \n",
    "def extract_compressed_file(compressed_file_path, decompressed_folder_path):\n",
    "    if compressed_file_path.endswith('.gz'):\n",
    "        with gzip.open(compressed_file_path, 'rb') as compressed_file:\n",
    "            with open(decompressed_folder_path, 'wb') as decompressed_file:\n",
    "                decompressed_file.write(compressed_file.read())\n",
    "    elif compressed_file_path.endswith('.zip'):\n",
    "        with zipfile.ZipFile(compressed_file_path, 'r') as zip_file:\n",
    "            zip_file.extractall(os.path.dirname(decompressed_folder_path))\n",
    "\n",
    "# Create folder if not exist\n",
    "mkdir(raw_logs_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_logs(identifier, egg, last_log, staging_folder):\n",
    "    eggs_minecraft = ('Vanilla Minecraft', 'Forge Minecraft', 'Paper', 'Spigot', 'Mohist')\n",
    "    eggs_ark = ('Ark: Survival Evolved',)\n",
    "    all_eggs = eggs_minecraft + eggs_ark\n",
    "\n",
    "    folder_logs = {\n",
    "        'Vanilla Minecraft': '/logs/',\n",
    "        'Forge Minecraft': '/logs/',\n",
    "        'Paper': '/logs/',\n",
    "        'Spigot': '/logs/',\n",
    "        'Mohist': '/logs/',\n",
    "        'Ark: Survival Evolved': '/ShooterGame/Saved/Logs/'\n",
    "    }\n",
    "\n",
    "    log_pattern = {\n",
    "        'Vanilla Minecraft': r'^\\d{4}-\\d{2}-\\d{2}-\\d.*', # yyyy-mm-dd-n*\n",
    "        'Forge Minecraft': r'^\\d{4}-\\d{2}-\\d{2}-\\d.*',\n",
    "        'Paper': r'^\\d{4}-\\d{2}-\\d{2}-\\d.*',\n",
    "        'Spigot': r'^\\d{4}-\\d{2}-\\d{2}-\\d.*',\n",
    "        'Mohist': r'^\\d{4}-\\d{2}-\\d{2}-\\d.*',\n",
    "        'Ark: Survival Evolved': r'^\\d{4}-\\d{2}-\\d{2}-\\d.*' #ServerGame.316.2023.11.17_19.25.10.log or ShooterGame-backup-2023.11.16-21.13.36.log - not sure \n",
    "    }\n",
    "\n",
    "    compression_ext = ('.gz', '.zip')\n",
    "\n",
    "    if egg in all_eggs:\n",
    "        # Create a folder as staging area for each server\n",
    "        folder_server_dir = os.path.join(staging_folder, identifier)\n",
    "        mkdir(folder_server_dir)\n",
    "\n",
    "        # Get the last log date to download only necessary logs\n",
    "        if last_log is not None:\n",
    "            last_log_date = last_log.strftime('%Y-%m-%d')\n",
    "        else:\n",
    "            # Set a default value to download all logs in case when last log date is not available\n",
    "            last_log_date = '2000-01-01'\n",
    "\n",
    "        # Get a list of the logs inside the server\n",
    "        try:\n",
    "            log_files = api_cli.client.servers.files.list_files(identifier, folder_logs[egg])\n",
    "            log_files_data = log_files.get('data', [])\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response.status_code == 404 or e.response.status_code == 504:\n",
    "                print(f\"Server {identifier} not found in Pterodactyl.\")\n",
    "                return None # Skip this and continue to the next iteration\n",
    "\n",
    "        # If log_files_data is not empty, sort the list of logs based on the date naming\n",
    "        if log_files_data:\n",
    "            list_logs = [file['attributes']['name'] for file in log_files_data if re.match(log_pattern[egg], file['attributes']['name'])]\n",
    "            sorted_list_logs = sort_list_logs(list_logs)\n",
    "            sorted_list_logs_date = [item[:10] for item in sorted_list_logs]\n",
    "        else:\n",
    "            print(f\"No log files found in server {identifier} with the client API.\")\n",
    "\n",
    "        # Select only a list of downloadable_logs which are new; after the last_log_date\n",
    "        try:\n",
    "            index_last_log = last_index(sorted_list_logs_date, last_log_date)\n",
    "        except:\n",
    "            index_last_log = -1\n",
    "        downloadable_logs = sorted_list_logs[index_last_log + 1:]\n",
    "\n",
    "        # Download all logs in the list downloadable_logs\n",
    "        list_download = [api_cli.client.servers.files.download_file(server_info[0], f'/logs/{log}') for log in downloadable_logs]\n",
    "        if list_download:\n",
    "            [urllib.request.urlretrieve(list_download[i], os.path.join(folder_server_dir, list_logs[i])) for i in range(len(list_download))]\n",
    "        print(f'Files downloaded: {len(list_download)}')\n",
    "\n",
    "        # Uncompressing files if needed\n",
    "        for filename in os.listdir(folder_server_dir):\n",
    "            if filename.endswith(compression_ext):\n",
    "                compressed_file_path = os.path.join(folder_server_dir, filename)\n",
    "                decompressed_file_path = os.path.splitext(compressed_file_path)[0]  # Remove the last extension\n",
    "\n",
    "                # Uncompress the file\n",
    "                extract_compressed_file(compressed_file_path, decompressed_file_path)\n",
    "\n",
    "                # Delete the compressed file\n",
    "                os.remove(compressed_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract logs from each active Minecraft Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_PTERODACTYL = 'pterodactyl'\n",
    "SCHEMA_MINECRAFT = 'minecraft'\n",
    "\n",
    "eggs_ready = ('Vanilla Minecraft', 'Forge Minecraft', 'Paper', 'Mohist') # Vanilla Bedrock is still not ready to be processed\n",
    "\n",
    "# Get server information\n",
    "engine = create_engine(connection)\n",
    "with engine.connect() as conn:\n",
    "    list_servers = conn.execute(text(f'SELECT servers.identifier, eggs.name, last_log_date.last_date FROM {SCHEMA_PTERODACTYL}.servers JOIN {SCHEMA_PTERODACTYL}.eggs ON eggs.id = servers.egg_id LEFT JOIN {SCHEMA_MINECRAFT}.last_log_date ON last_log_date.server_identifier = servers.identifier WHERE servers.is_active = true and servers.nest_id = 1'))\n",
    "\n",
    "for server_info in list_servers:\n",
    "\n",
    "    download_logs(identifier = server_info[0], egg = server_info[1], last_log = server_info[2], staging_folder = raw_logs_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation from logs data to information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_extension = '.log'\n",
    "\n",
    "folder = os.listdir(raw_logs_folder)[0]\n",
    "folder_server_dir = os.path.join(raw_logs_folder, folder)\n",
    "folder_server_dir\n",
    "\n",
    "log_files = [log for log in os.listdir(folder_server_dir) if log.endswith(log_extension)]\n",
    "log_files = sort_list_logs(log_files)\n",
    "\n",
    "if log_files:\n",
    "\n",
    "    all_logs = \"\"\n",
    "    for log_file in log_files:\n",
    "        with open(os.path.join(folder_server_dir, log_file), 'r', encoding='utf-8') as file:\n",
    "            log_contents = file.read().split('\\n')\n",
    "            log_contents = \"\\n\".join([f'[{log_file[:-(2+len(log_extension))]}] ' + line for line in log_contents if line.strip() != \"\"])\n",
    "            all_logs += log_contents + \"\\n\"\n",
    "all_logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re.findall(r'\\[(\\d{4}-\\d{2}-\\d{2})\\] \\[.*?(\\d{2}:\\d{2}:\\d{2})(?: INFO|)\\].*?: (.*)', '[2023-01-01] [21:29:59 INFO]: good\\n[2023-02-02] [14:38:37] [asdf/INFO]: good\\n[2023-03-03] [21:29:59] [Server thread/INFO] [minecraft/DedicatedServer]: Stopping the server')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_logs = pd.DataFrame(columns=['server_identifier', 'date', 'time', 'information', 'user', 'activity'])\n",
    "\n",
    "for folder in os.listdir(raw_logs_folder): \n",
    "    folder_server_dir = os.path.join(raw_logs_folder, folder)\n",
    "\n",
    "    # Read all logs as one\n",
    "    log_files = [log for log in os.listdir(folder_server_dir) if log.endswith('.log')]\n",
    "    log_files = sort_list_logs(log_files)\n",
    "\n",
    "    if log_files:\n",
    "\n",
    "        all_logs = \"\"\n",
    "        for log_file in log_files:\n",
    "            with open(os.path.join(folder_server_dir, log_file), 'r', encoding='utf-8') as file:\n",
    "                log_contents = file.read().split('\\n')\n",
    "                log_contents = \"\\n\".join([f'[{log_file[:-(2+len(log_extension))]}] ' + line for line in log_contents if line.strip() != \"\"])\n",
    "                all_logs += log_contents + \"\\n\"\n",
    "\n",
    "        # Transform information in meaningful information\n",
    "        pattern = r'\\[(\\d{4}-\\d{2}-\\d{2})\\] \\[.*?(\\d{2}:\\d{2}:\\d{2})(?: INFO|)\\].*?: (.*)'\n",
    "        matches = re.findall(pattern, all_logs)\n",
    "\n",
    "        # Create a list of dictionaries to store the extracted data\n",
    "        log_data = [{'server_identifier': folder, 'date': match[0], 'time': match[1], 'information': match[2]} for match in matches]\n",
    "        # Create a dataframe of the logs\n",
    "        df_logs = pd.DataFrame(log_data)\n",
    "        df_logs['user'] = None\n",
    "        df_logs['activity'] = None\n",
    "\n",
    "        # Add information when server started\n",
    "        index=df_logs[df_logs['information'].str.startswith(\"Starting minecraft server version\")].index\n",
    "        if not index.empty:\n",
    "            df_logs.loc[index, 'user'] = 'server'\n",
    "            df_logs.loc[index, 'activity'] = 'start'\n",
    "        # Add information when server stopped\n",
    "        index=df_logs[df_logs['information'].str.startswith(\"Stopping the server\")].index\n",
    "        if not index.empty:\n",
    "            df_logs.loc[index, 'user'] = 'server'\n",
    "            df_logs.loc[index, 'activity'] = 'stop'\n",
    "        # Add information when user get an achievement\n",
    "        index=df_logs[df_logs['information'].str.startswith(user)].index\n",
    "        if not index.empty:\n",
    "            df_logs.loc[index, 'user'] = user\n",
    "            df_logs.loc[index, 'activity'] = 'achievement'\n",
    "        # Add information when user login\n",
    "        index=df_logs[df_logs['information'].str.extract(r'(\\w+)\\[.*\\] logged in with entity id \\d+ at \\(.*\\)')].index\n",
    "        if not index.empty:\n",
    "            df_logs.loc[index, 'user'] = user\n",
    "            df_logs.loc[index, 'activity'] = 'login'\n",
    "        # Add information when user logout\n",
    "        index = df_logs[df_logs['information'].str.startswith(f'{user} left the game')].index\n",
    "        if not index.empty:\n",
    "            df_logs.loc[index, 'user'] = user\n",
    "            df_logs.loc[index, 'activity'] = 'logout'\n",
    "        # Delete duplicated activity of login and logout\n",
    "        index=df_logs[df_logs['information'].str.startswith(f'{user}[') | df_logs['information'].str.startswith(f'{user} lost connection: Disconnected')].index\n",
    "        if not index.empty:\n",
    "            df_logs.loc[index, 'user'] = None\n",
    "            df_logs.loc[index, 'activity'] = None\n",
    "        # Rows with no server/user activity is deleted\n",
    "        df_logs = df_logs.dropna(subset=['activity'])\n",
    "\n",
    "        # Save every log in df_all_logs for each iteration\n",
    "        df_all_logs = pd.concat([df_all_logs, df_logs], ignore_index=True)\n",
    "\n",
    "# Delete all logs inside each server folder\n",
    "for folder in os.listdir(raw_logs_folder):\n",
    "    folder_server_dir = os.path.join(raw_logs_folder, folder)\n",
    "    [os.remove(os.path.join(folder_server_dir, log)) for log in os.listdir(folder_server_dir) if log.endswith(log_extension)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YourTextHere12']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.findall(r'(\\w+)\\[.*\\] logged in with entity id \\d+ at \\(.*\\)', 'YourTextHere12[/186.79.124.37:35491] logged in with entity id 1 at ([world]1.3973069313844464, 143.0, 9.566801921811514)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>YourTextHere12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>YourTextHere12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>QuememirasWeyxd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>YourTextHere12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>YourTextHere12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4821</th>\n",
       "      <td>Indominux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4931</th>\n",
       "      <td>Indominux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5036</th>\n",
       "      <td>Indominux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5507</th>\n",
       "      <td>lujan34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5512</th>\n",
       "      <td>Indominux</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0\n",
       "120    YourTextHere12\n",
       "126    YourTextHere12\n",
       "129   QuememirasWeyxd\n",
       "139    YourTextHere12\n",
       "146    YourTextHere12\n",
       "...               ...\n",
       "4821        Indominux\n",
       "4931        Indominux\n",
       "5036        Indominux\n",
       "5507          lujan34\n",
       "5512        Indominux\n",
       "\n",
       "[78 rows x 1 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_logs['information'].str.extract(r'(\\w+)\\[.*\\] logged in with entity id \\d+ at \\(.*\\)').dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processed data into Data Warehouse (Postgres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Schema and Table names in Postgres\n",
    "SCHEMA = 'pterodactyl'\n",
    "TABLE = 'activity'\n",
    "\n",
    "# Connect to database and upload all new logs into table\n",
    "engine = create_engine(connection)\n",
    "with engine.connect() as conn:\n",
    "\n",
    "# Start a new transaction\n",
    "    trans = conn.begin()\n",
    "\n",
    "    try:\n",
    "        # Load all new activity into postgres\n",
    "        df_all_logs.to_sql(name = TABLE, schema = SCHEMA, con = conn, if_exists='append', index=False)\n",
    "        # Commit the transaction\n",
    "        trans.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        # Rollback the transaction on exception\n",
    "        print('!!! [ERROR IN DATABASE QUERIES] !!!')\n",
    "        trans.rollback()\n",
    "        print('Transaction has been rolled back')\n",
    "        print(f'Error occurred during transaction:\\n{e}')\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
